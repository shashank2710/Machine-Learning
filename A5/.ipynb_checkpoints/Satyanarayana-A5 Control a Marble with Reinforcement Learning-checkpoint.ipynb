{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Control a Marble with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shashank Satyanarayana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will be modifying the reinforcement learning code used to solve the dynamic marble problem.  You will be solving a more complex version of the marble problem in which a goal position is specified as a new state variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the code in the notebook for lecture notes 24, and the associated neural network and mlutilities that you can download in [this tar file](http://www.cs.colostate.edu/~anderson/cs445/notebooks/A5.tar).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the code from Lecture Notes 24 to change the\n",
    "reinforcement learning problem to one for which the goal position of\n",
    "the marble can be varied. In other words, your new code must train one Q network for the marble problem with a variable goal. \n",
    "\n",
    "Use the plotting code from Lecture 24 to show your results.  In addition to those plots, create nine additional versions of the lower-right plot for goal positions at 1, 2, 3, 4, 5, 6, 7, 8, and 9.\n",
    "\n",
    "The general approach is as follows.\n",
    "\n",
    "The state of the marble is given by $(x_t,\n",
    "\\dot{x_t})$.  In this new problem, the state will be $(x_t,\n",
    "\\dot{x_t}, g_t)$, where $g_t$ is the goal at time $t$.  Modify the\n",
    "`initialState` function to randomly choose a new goal $g_t$ to be a\n",
    "random value between 1 and 9. Modify `nextState` so that the goal\n",
    "value remains the same in the new state as it was in the old state.  You will also\n",
    "have to parameterize the reinforcement function to depend on the\n",
    "current $g_t$ value.\n",
    "\n",
    "Many of the figures will still make sense.  Include them in your\n",
    "report.  The contour (and surface)\n",
    "plots and the plot that tests the marble's behavior for various intial\n",
    "positions must be produced for a single value of the goal.  Generate\n",
    "several versions of the contour and surface plots and the test plots for goals of 1, 5, and 9 to show how\n",
    "they vary when the goal is varied.\n",
    "\n",
    "Experiment with values of the parameters, such as the number of trials, number of\n",
    "steps per trial, number of SCG iterations in each train call, number of hidden units, finalEpsilon, and gamma.  Try\n",
    "to find values that result in a trained network that controls the\n",
    "marble well for most of the goal settings.\n",
    "Discuss your experience with finding good parameter values.\n",
    "\n",
    "Discuss the changes you tried and the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import neuralnetworksA4 as nn\n",
    "global g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "actions = (-1, 0, 1)   # Possible actions\n",
    "\n",
    "def reinforcement(s,s1):\n",
    "    # return 0 if abs(s1[0]-goal) < 1 else -1\n",
    "    return -abs(s1[0] - s[2])\n",
    "\n",
    "def initialState():\n",
    "    global goal\n",
    "    goal = random.randint(1,10)\n",
    "    return np.array([10*np.random.random_sample(), 0.0,goal])\n",
    "\n",
    "def nextState(s,a,goal):\n",
    "    s = copy(s)   # s[0] is position, s[1] is velocity. a is -1, 0 or 1\n",
    "    deltaT = 0.1                           # Euler integration time step\n",
    "    s[0] += deltaT * s[1]                  # Update position\n",
    "    s[1] += deltaT * (2 * a - 0.2 * s[1])  # Update velocity. Includes friction\n",
    "    s[2] = goal\n",
    "    if s[0] < 0:        # Bound next position. If at limits, set velocity to 0.\n",
    "        s = [0,0,s[2]]\n",
    "    elif s[0] > 10:\n",
    "        s = [10,0,s[2]]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilonGreedy(nnetQ, state, actions, epsilon):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Random Move\n",
    "        action = np.random.choice(actions)\n",
    "    else:\n",
    "        # Greedy Move\n",
    "        Qs = [nnetQ.use(np.hstack((state,a)).reshape((1, -1))) for a in actions]\n",
    "        ai = np.argmax(Qs)\n",
    "        action = actions[ai]\n",
    "    Q = nnetQ.use(np.hstack((state,action)).reshape((1, -1)))\n",
    "    return action, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeSamples(nnet, initialStateF, nextStateF, reinforcementF,\n",
    "                validActions, numSamples, epsilon):\n",
    "    global goal\n",
    "    X = np.zeros((numSamples, nnet.ni))\n",
    "    R = np.zeros((numSamples, 1))\n",
    "    Qn = np.zeros((numSamples, 1))\n",
    "\n",
    "    s = initialStateF()\n",
    "    s = nextStateF(s, 0, goal)        # Update state, sn from s and a\n",
    "    a, _ = epsilonGreedy(nnet, s, validActions, epsilon)\n",
    "\n",
    "    # Collect data from numSamples steps\n",
    "    for step in range(numSamples):\n",
    "        sn = nextStateF(s, a, goal)        # Update state, sn from s and a\n",
    "        rn = reinforcementF(s, sn)   # Calculate resulting reinforcement\n",
    "        an, qn = epsilonGreedy(nnet, sn, validActions, epsilon) # Forward pass for time t+1\n",
    "        X[step, :] = np.hstack((s,a))\n",
    "        R[step, 0] = rn\n",
    "        Qn[step, 0] = qn\n",
    "        # Advance one time step\n",
    "        s, a = sn, an\n",
    "\n",
    "    return (X, R, Qn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "def plotStatus(net,trial,epsilonTrace,rtrace):\n",
    "    plt.subplot(4,3,1)\n",
    "    plt.plot(epsilonTrace[:trial+1])\n",
    "    plt.ylabel(\"Random Action Probability ($\\epsilon$)\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.subplot(4,3,2)\n",
    "    plt.plot(X[:,0])\n",
    "    plt.plot([0,X.shape[0]], [goal,goal],'--',alpha=0.5,lw=5)\n",
    "    plt.ylabel(\"$x$\")\n",
    "    plt.ylim(-1,11)\n",
    "    #qs = [[net.use([s,0,a]) for a in actions] for s in range(11)]\n",
    "    qs = net.use(np.array([[s,0,goal,a] for a in actions for s in range(11)]))\n",
    "    #print np.hstack((qs,-1+np.argmax(qs,axis=1).reshape((-1,1))))\n",
    "    plt.subplot(4,3,3)\n",
    "    acts = [\"L\",\"0\",\"R\"]\n",
    "    actsiByState = np.argmax(qs.reshape((len(actions),-1)),axis=0)\n",
    "    for i in range(11):\n",
    "        plt.text(i,0,acts[actsiByState[i]])\n",
    "        plt.xlim(-1,11)\n",
    "        plt.ylim(-1,1)\n",
    "    plt.text(2,0.2,\"Policy for Zero Velocity\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(4,3,4)\n",
    "    plt.plot(rtrace[:trial+1],alpha=0.5)\n",
    "    #plt.plot(np.convolve(rtrace[:trial+1],np.array([0.02]*50),mode='valid'))\n",
    "    binSize = 20\n",
    "    if trial+1 > binSize:\n",
    "        # Calculate mean of every bin of binSize reinforcement values\n",
    "        smoothed = np.mean(rtrace[:int(trial/binSize)*binSize].reshape((int(trial/binSize),binSize)),axis=1)\n",
    "        plt.plot(np.arange(1,1+int(trial/binSize))*binSize,smoothed)\n",
    "    plt.ylabel(\"Mean reinforcement\")\n",
    "    plt.subplot(4,3,5)\n",
    "    plt.plot(X[:,0],X[:,1])\n",
    "    plt.plot(X[0,0],X[0,1],'o')\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$\\dot{x}$\")\n",
    "    plt.fill_between([4,6],[-5,-5],[5,5],color=\"red\",alpha=0.3)\n",
    "    plt.xlim(-1,11)\n",
    "    plt.ylim(-5,5)\n",
    "    plt.subplot(4,3,6)\n",
    "    net.draw([\"$x$\",\"$\\dot{x}$\",\"$a$\"],[\"Q\"])\n",
    "    \n",
    "    plt.subplot(4,3,7)\n",
    "    n = 20\n",
    "    positions = np.linspace(0,10,n)\n",
    "    velocities =  np.linspace(-5,5,n)\n",
    "    xs,ys = np.meshgrid(positions,velocities)\n",
    "    #states = np.vstack((xs.flat,ys.flat)).T\n",
    "    #qs = [net.use(np.hstack((states,np.ones((states.shape[0],1))*act))) for act in actions]\n",
    "    xsflat = xs.flat\n",
    "    ysflat = ys.flat\n",
    "    qs = net.use(np.array([[xsflat[i],ysflat[i],a] for a in actions for i in range(len(xsflat))]))\n",
    "    #qs = np.array(qs).squeeze().T\n",
    "    qs = qs.reshape((len(actions),-1)).T\n",
    "    qsmax = np.max(qs,axis=1).reshape(xs.shape)\n",
    "    cs = plt.contourf(xs,ys,qsmax)\n",
    "    plt.colorbar(cs)\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$\\dot{x}$\")\n",
    "    plt.title(\"Max Q\")\n",
    "    plt.subplot(4,3,8)\n",
    "    acts = np.array(actions)[np.argmax(qs,axis=1)].reshape(xs.shape)\n",
    "    cs = plt.contourf(xs,ys,acts,[-2, -0.5, 0.5, 2])\n",
    "    plt.colorbar(cs)\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$\\dot{x}$\")\n",
    "    plt.title(\"Actions\")\n",
    "    \n",
    "    s = plt.subplot(4,3,10)\n",
    "    rect = s.get_position()\n",
    "    ax = Axes3D(plt.gcf(),rect=rect)\n",
    "    ax.plot_surface(xs,ys,qsmax,cstride=1,rstride=1,cmap=cm.jet,linewidth=0)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$\\dot{x}$\")\n",
    "    #ax.set_zlabel(\"Max Q\")\n",
    "    plt.title(\"Max Q\")\n",
    "    \n",
    "    s = plt.subplot(4,3,11)\n",
    "    rect = s.get_position()\n",
    "    ax = Axes3D(plt.gcf(),rect=rect)\n",
    "    ax.plot_surface(xs,ys,acts,cstride=1,rstride=1,cmap=cm.jet,linewidth=0)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$\\dot{x}$\")\n",
    "    #ax.set_zlabel(\"Action\")\n",
    "    plt.title(\"Action\")\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testIt(Qnet,nTrials,nStepsPerTrial):\n",
    "    xs = np.linspace(0,10,nTrials)\n",
    "    plt.subplot(4,3,12)\n",
    "    for x in xs:\n",
    "        s = [x,0, goal] ## 0 velocity\n",
    "        xtrace = np.zeros((nStepsPerTrial,2))\n",
    "        for step in range(nStepsPerTrial):\n",
    "            a,_ = epsilonGreedy(Qnet,s,actions,0.0) # epsilon = 0\n",
    "            s = nextState(s,a, goal)\n",
    "            xtrace[step,:] = s\n",
    "        plt.plot(xtrace[:,0],xtrace[:,1])\n",
    "        plt.xlim(-1,11)\n",
    "        plt.ylim(-5,5)\n",
    "        plt.plot([goal, goal],[-5,5],'--',alpha=0.5,lw=5)\n",
    "        plt.ylabel('$\\dot{x}$')\n",
    "        plt.xlabel('$x$')\n",
    "        plt.title('State Trajectories for $\\epsilon=0$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setupStandardization(net, Xmeans, Xstds, Tmeans, Tstds):\n",
    "    net.Xmeans = Xmeans\n",
    "    net.XstdsFixed = Xstds\n",
    "    net.Xconstant = [False] * len(Xmeans)\n",
    "    net.TstdsFixed = net.Tstds = Tstds\n",
    "    net.Tmeans = Tmeans\n",
    "    net.Tconstant = [False] * len(Tstds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilonDecay is 0.990831944893\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'goal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e2258f2a16fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Collect nStepsPerRep samples of X, R, Qn, and Q, and update epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnetQ\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0minitialState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreinforcement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnStepsPerTrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mnnetQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mQn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnIterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnSCGIterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d5ff370b568b>\u001b[0m in \u001b[0;36mmakeSamples\u001b[0;34m(nnet, initialStateF, nextStateF, reinforcementF, validActions, numSamples, epsilon)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialStateF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnextStateF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# Update state, sn from s and a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilonGreedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidActions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'goal' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd22323f7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "gamma = 0.8        # discount factor\n",
    "nTrials = 500         # number of repetitions of makeSamples-updateQ loop\n",
    "nStepsPerTrial = 1000 # number of steps between new random initial states\n",
    "nSCGIterations = 4  # maximum number of SCG iterations\n",
    "finalEpsilon = 0.01 # value of epsilon at end of simulation. Decay rate is calculated\n",
    "epsilonDecay =  np.exp(np.log(finalEpsilon)/(nTrials)) # to produce this final value\n",
    "print(\"epsilonDecay is\",epsilonDecay)\n",
    "\n",
    "nh = 10           # number of hidden units\n",
    "nnetQ = nn.NeuralNetwork(4, nh, 1)  # ((0,10),(-3,3),(-1,1)))\n",
    "# Inputs are position (1 to 10) velocity (-3 to 3) and action (-1, 0, or 1)\n",
    "setupStandardization(nnetQ, [5, 0, 5, 0], [2, 2, 2, 0.5], [0], [1])\n",
    "\n",
    "epsilon = 1         # initial epsilon value\n",
    "epsilonTrace = np.zeros(nTrials)\n",
    "rtrace = np.zeros(nTrials)\n",
    "for trial in range(nTrials):\n",
    "    # Collect nStepsPerRep samples of X, R, Qn, and Q, and update epsilon\n",
    "    \n",
    "    X, R, Qn = makeSamples(nnetQ,  initialState, nextState, reinforcement, actions, nStepsPerTrial, epsilon)\n",
    "    nnetQ.train(X, R + gamma*Qn, nIterations=nSCGIterations)\n",
    "\n",
    "    # X,R,Qn,Q,epsilon = getSamples(Qnet,actions,nStepsPerTrial,epsilon)\n",
    "    # Rest is for plotting\n",
    "    epsilonTrace[trial] = epsilon\n",
    "    epsilon *= epsilonDecay\n",
    "    rtrace[trial] = np.mean(R)\n",
    "    if True and (trial+1 == nTrials or trial % (nTrials/40) == 0):\n",
    "        fig.clf()\n",
    "        plotStatus(nnetQ, trial, epsilonTrace, rtrace)\n",
    "        testIt(nnetQ, 10, 500)\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "    \n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=[1,1,1]\n",
    "s[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no automatic grading code for this notebook.  Do your best to show code and results you used to find good parameter values, to train your successful Q network, and to demonstrate that it is successful.\n",
    "\n",
    "Include text to explain what you did, how successful it was, and any problems you encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One point of extra credit will be given for completing each of the following things.\n",
    "\n",
    "* Change the marble's world from one dimension to two. Add graphs of the marble's movement in the two-dimensional plane.\n",
    "* Increase the number of valid actions from three to seven and discuss the difference between the required runs and these new runs.\n",
    "* Add a variable wind as a force on the marble, along with another state variable that indicates wind speed and direction.\n",
    "* Add a second marble with its own RL agent.  Add negative reinforcement if they bump into each other.\n",
    "* Add areas of increased friction to the track.\n",
    "\n",
    "For all of these, demonstrate the effects and write about what you observe.  Full extra credit points will only be awarded if you clearly describe your work and results for each point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not include this section in your notebook.\n",
    "\n",
    "Name your notebook ```Lastname-A5.ipynb```.  So, for me it would be ```Anderson-A5.ipynb```.  Submit the file using the ```Assignment 5``` link on [Canvas](https://colostate.instructure.com/courses/41327).\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
