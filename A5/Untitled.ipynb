{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'nhs' and 'no'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7f0f032d181f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0mnh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m \u001b[0mqnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0mqnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInputRanges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'nhs' and 'no'"
     ]
    }
   ],
   "source": [
    "import neuralnetworksA4 as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import copy\n",
    "\n",
    "validActions = np.array([ -1, 0, 1])\n",
    "\n",
    "def initialState(trial):\n",
    "    global goal\n",
    "    if trial == 0:\n",
    "        goal = random.randint(1,10)\n",
    "    return np.array([10*np.random.random_sample(), 3*(0.5-np.random.random_sample()),goal])\n",
    "\n",
    "def nextState(s,a,goal):\n",
    "    s = copy.copy(s)   # s[0] is position, s[1] is velocity. a is -1, 0 or 1\n",
    "    deltaT = 0.1                           # Euler integration time step\n",
    "    s[0] += deltaT * s[1]                  # Update position\n",
    "    s[1] += deltaT * (2 * a - 0.2 * s[1])  # Update velocity. Includes friction\n",
    "    s[2] = goal\n",
    "    if s[0] < 0:        # Bound next position. If at limits, set velocity to 0.\n",
    "        s = np.array([0,0,s[2]])\n",
    "    elif s[0] > 10:\n",
    "        s = np.array([10,0,s[2]])\n",
    "    return s\n",
    "\n",
    "def reinforcement(s):  # s is new state\n",
    "    return 0 if abs(s[0]-s[2]) < 1 else -0.1\n",
    "\n",
    "def policy(qnet, state, epsilon):\n",
    "    if np.random.rand(1) < epsilon:\n",
    "        actioni = np.random.randint(validActions.shape[0])\n",
    "    else:\n",
    "        inputs = np.hstack(( np.tile(state, (validActions.shape[0], 1)), validActions.reshape((-1,1))))\n",
    "        qs = qnet.use(inputs)\n",
    "        actioni = np.argmax(qs)\n",
    "    return validActions[actioni]\n",
    "\n",
    "\n",
    "def makeSamples(qnet, nStepsPerStart,trial,inval):\n",
    "    global goal\n",
    "    samples = []\n",
    "    state = initialState(trial)\n",
    "    act = policy(qnet, state, epsilon)\n",
    "    oldact = act\n",
    "    print(goal)\n",
    "    for iStep in range(nStepsPerStart):\n",
    "        newState = nextState(state, act,goal)\n",
    "        r = reinforcement(newState)\n",
    "        newAct = policy(qnet, newState, epsilon)\n",
    "        # SARSA\n",
    "        samples.append(state.tolist() + [act, r] + newState.tolist() + [newAct])\n",
    "        state = newState\n",
    "        oldact = act\n",
    "        act = newAct\n",
    "    return np.array(samples)\n",
    "    \n",
    "\n",
    "def plotStatus(qnet, X, R, trial, epsilonTrace, rtrace):\n",
    "    \n",
    "    plt.subplot(4,3,1)\n",
    "    plt.plot(epsilonTrace[:trial+1])\n",
    "    plt.ylabel(\"Random Action Probability ($\\epsilon$)\")\n",
    "    plt.ylim(0,1)\n",
    "    \n",
    "    plt.subplot(4,3,2)\n",
    "    plt.plot(X[:,0])\n",
    "    plt.plot([0,X.shape[0]], [goal,goal],'--',alpha=0.5,lw=5)\n",
    "    plt.ylabel(\"$x$\")\n",
    "    plt.ylim(-1,11)\n",
    "    qs = qnet.use(np.array([[s,0,goal,a] for a in validActions for s in range(11)]))\n",
    "\n",
    "    \n",
    "    plt.subplot(4,3,3)\n",
    "    acts = [\"L\",\"0\",\"R\"]\n",
    "    actsiByState = np.argmax(qs.reshape((len(validActions),-1)),axis=0)\n",
    "    for i in range(11):\n",
    "        plt.text(i,0,acts[actsiByState[i]])\n",
    "        plt.xlim(-1,11)\n",
    "        plt.ylim(-1,1)\n",
    "    plt.text(2,0.2,\"Policy for Zero Velocity\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(4,3,4)\n",
    "    plt.plot(rtrace[:trial+1],alpha=0.5)\n",
    "    binSize = 20\n",
    "    if trial+1 > binSize:\n",
    "        smoothed = np.mean(rtrace[:int(trial/binSize)*binSize].reshape((int(trial/binSize),binSize)),axis=1)\n",
    "        plt.plot(np.arange(1,1+int(trial/binSize))*binSize,smoothed)\n",
    "    plt.ylabel(\"Mean reinforcement\")\n",
    "    \n",
    "    \n",
    "    plt.subplot(4,3,5)\n",
    "    plt.plot(X[:,0],X[:,1])\n",
    "    plt.plot(X[0,0],X[0,1],'o')\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$\\dot{x}$\")\n",
    "    plt.fill_between([goal-1,goal+1],[-5,-5],[5,5],color=\"red\",alpha=0.3)\n",
    "    plt.xlim(-1,11)\n",
    "    plt.ylim(-5,5)\n",
    "\n",
    "    plt.subplot(4,3,6)\n",
    "    qnet.draw([\"$x$\",\"$\\dot{x}$\",\"$a$\"],[\"Q\"])\n",
    "\n",
    "    plt.subplot(4,3,7)\n",
    "    n = 20\n",
    "    positions = np.linspace(0,10,n)\n",
    "    velocities =  np.linspace(-5,5,n)\n",
    "    xs,ys = np.meshgrid(positions,velocities)\n",
    "    xsflat = xs.flat\n",
    "    ysflat = ys.flat\n",
    "    qs = qnet.use(np.array([[xsflat[i],ysflat[i],goal,a] for a in validActions for i in range(len(xsflat))]))\n",
    "    qs = qs.reshape((len(validActions),-1)).T\n",
    "    qsmax = np.max(qs,axis=1).reshape(xs.shape)\n",
    "    cs = plt.contourf(xs,ys,qsmax)\n",
    "    plt.colorbar(cs)\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$\\dot{x}$\")\n",
    "    plt.title(\"Max Q\")\n",
    "   \n",
    "    plt.subplot(4,3,8)\n",
    "    acts = np.array(validActions)[np.argmax(qs,axis=1)].reshape(xs.shape)\n",
    "    cs = plt.contourf(xs,ys,acts,[-2, -0.5, 0.5, 2])\n",
    "    plt.colorbar(cs)\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$\\dot{x}$\")\n",
    "    plt.title(\"Actions\")\n",
    "\n",
    "    s = plt.subplot(4,3,10)\n",
    "    rect = s.get_position()\n",
    "    ax = Axes3D(plt.gcf(),rect=rect)\n",
    "    ax.plot_surface(xs,ys,qsmax,cstride=1,rstride=1,cmap=cm.viridis,linewidth=0)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$\\dot{x}$\")\n",
    "    plt.title(\"Max Q\")\n",
    "\n",
    "    s = plt.subplot(4,3,11)\n",
    "    rect = s.get_position()\n",
    "    ax = Axes3D(plt.gcf(),rect=rect)\n",
    "    ax.plot_surface(xs,ys,acts,cstride=1,rstride=1,cmap=cm.viridis,linewidth=0)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$\\dot{x}$\")\n",
    "    plt.title(\"Action\")    \n",
    "    \n",
    "    \n",
    "def testIt(qnet,nTrials,nStepsPerTrial,inval):\n",
    "    xs = np.linspace(0,10,nTrials)\n",
    "    plt.subplot(4,3,12)\n",
    "    for x in xs:\n",
    "        s = [x,0,goal] ## 0 velocity\n",
    "        xtrace = np.zeros((nStepsPerTrial,3))\n",
    "        for step in range(nStepsPerTrial):\n",
    "            a = policy(qnet, s, 0.0)  # epsilon = 0\n",
    "            s = nextState(s,a,goal)\n",
    "            xtrace[step,:] = s\n",
    "        plt.plot(xtrace[:,0],xtrace[:,1])\n",
    "        plt.xlim(-1,11)\n",
    "        plt.ylim(-5,5)\n",
    "        plt.plot([goal,goal],[-5,5],'--',alpha=0.5,lw=5)\n",
    "        plt.ylabel('$\\dot{x}$')\n",
    "        plt.xlabel('$x$')\n",
    "        plt.title('State Trajectories for $\\epsilon=0$ and Goal = %s'%(goal))\n",
    "        \n",
    "gamma = 0.999\n",
    "nTrials = 400\n",
    "nStepsPerTrial = 700 \n",
    "nSCGIterations = 40\n",
    "\n",
    "nh = [4,4]\n",
    "qnet = nn.NeuralNetwork([4], nh , [1])  \n",
    "qnet.setInputRanges(( (0, 10), (-3, 3), (0,10),(-1,1)))\n",
    "\n",
    "\n",
    "for inval in range(0,4,1):\n",
    "    epsilon = 1\n",
    "    finalEpsilon = 0.03\n",
    "    epsilonDecay = np.exp(np.log(finalEpsilon)/(nTrials))  # to produce this final value\n",
    "    epsilonTrace = np.zeros(nTrials)\n",
    "    rtrace = np.zeros(nTrials)\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    for trial in range(nTrials):\n",
    "        # Collect nStepsPerRep samples of X, R, Qn, and Q, and update epsilon\n",
    "        samples = makeSamples(qnet, nStepsPerTrial,trial,inval)\n",
    "        ns = 3\n",
    "        \n",
    "        \n",
    "gamma = 0.999\n",
    "nTrials = 400\n",
    "nStepsPerTrial = 700 \n",
    "nSCGIterations = 40\n",
    "\n",
    "nh = [4,4]\n",
    "qnet = nn.NeuralNetwork([4],nh,[1])  \n",
    "qnet.setInputRanges(( (0, 10), (-3, 3), (0,10),(-1,1)))\n",
    "\n",
    "\n",
    "for inval in range(0,1,1):\n",
    "    epsilon = 1\n",
    "    finalEpsilon = 0.03\n",
    "    epsilonDecay = np.exp(np.log(finalEpsilon)/(nTrials))  # to produce this final value\n",
    "    epsilonTrace = np.zeros(nTrials)\n",
    "    rtrace = np.zeros(nTrials)\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    for trial in range(nTrials):\n",
    "        # Collect nStepsPerRep samples of X, R, Qn, and Q, and update epsilon\n",
    "        samples = makeSamples(qnet, nStepsPerTrial,trial,inval)\n",
    "        ns = 3\n",
    "        na = 1\n",
    "        X = samples[:, :ns+na]\n",
    "        R = samples[:, ns+na:ns+na+1]\n",
    "        nextX = samples[:, ns+na+1:]\n",
    "        nextQ = qnet.use(nextX)\n",
    "\n",
    "        qnet.train(X, R + gamma * nextQ, nIterations = nSCGIterations)\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon *= epsilonDecay\n",
    "\n",
    "        # Rest is for plotting\n",
    "        epsilonTrace[trial] = epsilon\n",
    "        rtrace[trial] = np.mean(R)\n",
    "\n",
    "        if trial % (nTrials//10) == 0 or trial == nTrials-1:\n",
    "            plt.clf()\n",
    "            plotStatus(qnet, X, R, trial,epsilonTrace,rtrace)\n",
    "            testIt(qnet,10,500,inval)\n",
    "            clear_output(wait=False)\n",
    "            display(fig);\n",
    "            plt.pause(0.01)\n",
    "\n",
    "        # print('Trial',trial,'mean R',np.mean(R))\n",
    "    clear_output(wait=False)\n",
    "\n",
    "        \n",
    "na = 1\n",
    "X = samples[:, :ns+na]\n",
    "R = samples[:, ns+na:ns+na+1]\n",
    "nextX = samples[:, ns+na+1:]\n",
    "nextQ = qnet.use(nextX)\n",
    "\n",
    "qnet.train(X, R + gamma * nextQ, nIterations = nSCGIterations)\n",
    "\n",
    "# Decay epsilon\n",
    "epsilon *= epsilonDecay\n",
    "\n",
    "# Rest is for plotting\n",
    "epsilonTrace[trial] = epsilon\n",
    "rtrace[trial] = np.mean(R)\n",
    "\n",
    "if trial % (nTrials//10) == 0 or trial == nTrials-1:\n",
    "            plt.clf()\n",
    "            testIt(qnet,10,500,inval)\n",
    "            clear_output(wait=False)\n",
    "            display(fig);\n",
    "            plt.pause(0.01)\n",
    "\n",
    "        # print('Trial',trial,'mean R',np.mean(R))\n",
    "clear_output(wait=False)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
